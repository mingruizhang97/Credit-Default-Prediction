# Credit-Default-Prediction
Credit Default Prediction Project
# Motivation
Credit cards are a widely used payment method in modern life. It saves us from carrying cash, and it makes a full purchase in advance with a pre-approved credit limit. However, credit default has been a long-term issue for credit card issuers. Thus, card issuers are eager to know beforehand whether customers will pay back on time or not so that they can improve lending decisions and handle risk management better.
For this project, the dataset contains over 45,000 credit card accounts’ activities, including delinquency, spending, payment, balance, and risk. We applied machine learning techniques to this dataset to predict the probability that a customer does not pay back in a certain time range. In this phase,  compared with phase 1, we initially upgraded our new splitting strategy in order to make our baseline prediction more accurate. Moreover, we designed a system based on the Amazon Web Service that enables clients to get the prediction directly by uploading their customers’ data. In addition, we also tried deep learning methods such as the Gated Recurrent Unit model and Long-Short Term Memory networks.
# Dataset
American Express provides a dataset containing 458,913 customers’ monthly profiles, including 189 features which are distributed unevenly into 5 categories: Delinquency, Spending, Payment, Balance, and Risk (See Figure 1).
Considering customers’ privacy, data masking has been implemented upon all variables except user ID and statement data, so that variable names can only be recognized as a member in one of the categories above. Thus, it’s nearly impossible to get a realistic interpretation of the data for preprocessing or feature engineering. Statistical feature analysis like mutual correlations and information values can be used in this phase.  
# Methodology and Project Diagram
For low-risk problems, we mainly focused on the new splitting method so that we can make the prediction more accurate. And then we apply the same machine learning model (Logistic regression) into the new splitted training data and test data to compare the performance with that in the phase1. For medium-risk problems, we explored how to design a cloud system embedding database management, data storage, serverless applications, and cloud computing. The target will be having a smooth pipeline to simplify clients’ requests of getting predictions based on users’ statement history. For high-risk problems, we challenged the most general solution with upgraded models with deep learning models like the GRU and  the LSTM. The diagram presents our workflow can be found in Figure 2.
## Low-risk: New Splitting Method
Splitting Method In the Phase 1, before training the machine learning model, we splitted the dataset randomly, but for each customer, it might has more than one credit statement so that random-splitting method could cause one customer’s information to appeared not only in the training dataset but also in the test dataset, which is likely to decrease the accuracy. Therefore, we split the dataset based on the customer id. After the new splitting, there are 423,734  samples and 362, 825 unique customers in the training dataset, and there are 105,794 samples and 90,706 unique customers in the test dataset.

Performance Comparison After splitting the dataset, we did the same process in phase 1. We initially did the feature selection and then used the weight of evidence transformation. Finally, we applied the logistic regression model into the new training dataset to evaluate its performance. We can see the results in Figure 3 and Figure 4.  For the Kolmogorov-Smirnov (KS), it increases by 31% to reach 51%. For the AUC value, it also reaches 0.91.
## Medium-risk Problem:  Cloud System Design
With the cloud system getting more and more popular, how to implement AWS services and connect them as an automated pipeline can be an important component of a practical project which is not only modeling but also the development of a product. We split the design into 4 sub-questions below and then combine them together into a complete architecture shown in Figure 5. 
Database Management and Data Storage: Considering the key-value structure of the dataset, a relational database can be an effective tool to manage data operations, including data upload, request, and update. Besides that, storage is also necessary for a convenient data archive as per regular cases. Thus, AWS RDS and AWS S3 bucket can be useful since they can communicate with each other and are compatible with other services in AWS.  
Modular Serverless Applications: Less dependent modular design is an essential consideration when designing a general system that is easy to disassemble and reassemble. AWS Lambda Function can effectively achieve this with its “serverless” feature. With Step Functions or other services like SNS and SQS, we can develop any custom pipeline as per requirements. In this project, we want to achieve the automatic trigger of computing tasks and the real-time notification to clients about job start and end. The composition of the Lambda Function and SNS can be an effective solution. 
Docker Container for Application Deployment: Docker container can provide developers with a strong tool packaging application environment and all necessary libraries together for universal deployment in various AWS instances. With AWS ECS, jobs that need to run on deployed applications can be distributed with custom settings or automatic scaling to realize fast and effective running. 
Cloud Computing with Sequential Jobs: AWS Batch can help developers assign job priority to each submitted task so that sequential running can be realized. Each task will use the result from the previous task as the input, and an error warning will be sent to clients if a specific task fails. 
## High-risk:  Model Upgrading with XGBoost
Gated Recurrent Model: We chose the Gated Recurrent Unit Model which offered a streamlined version of the LSTM memory cell that often achieves comparable performance but with the advantage of being faster to compute. We can see the example in Figure 9.  The batch size and epochs are relatively 768 and 15 and we choose the ReLu and LeakyReLu as the However, we failed to get the result because the number of predicted output is not the same as the number of samples feeded into the GRU. We cannot figure out what causes this to happen. We failed in this model.
LSTM model: Since the dataset contains customer credit statement history with date time, it is reasonable to assume the dataset has a time series attribute and whether the customer will have credit default in the future or not is related to time of history. Therefore, we decided to use LSTM to train the model. LSTM(Long Short-Term Memory) is a type of RNN(Recurrent neural network) architecture that is designed to handle the problem of vanishing gradient descent which commonly happens in RNN. It is highly useful to be implemented with time series data, as they can selectively remember or forget previous inputs based on the current input thanks to the forget gate in the architecture.

In the dataset, each row contains one customer’s credit statement sample. To fit correctly into the LSTM model as the input data, we transformed the original dataset into a 3d dataset with dimensions (customer ID, credit statement of that customer, credit statement feature space). The transformed training dataset has 269390 negative samples and 93435 positive samples which have data imbalanced issues. We computed class weights based on the frequency of each class in the training set using compute_class_weight and set class weight in the model. The LSTM model is structured as shown in Figure 6 We used binary cross entropy as the loss and evaluated the model’s performance based on AUC and recall score. To compare trade-off between the performance and computational time on varied batch sizes, We trained the model on 100 and 800 batch sizes. Figure 7 is the loss and metrics changes through the training process. Table 1 is the result of training time, recall and AUC on the test set. Figure 8 is the confusion matrix on the test set. From those three above, we can find that The performance for both models is almost the same and relatively good (AUC around 0.958) while models with 800 batch size have smaller training time.

## Conclusion:
The phase 2 project is an extension of phase 1 project and made the whole project complete. We tried the new and more reasonable split method on the current project, re-trained logistic regression model on new split dataset, tried to utilize LSTM and GRU deep learning model and proposed an organized cloud system design for future company use. The LSTM method can have better performance compared with logistic regression, but it has a high demand for time cost and computation resources. In the future, the company should spare more budget on cloud systems and computing if they aim to achieve higher accuracy and faster prediction speeds.


